{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60faaa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil, floor\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import resource_variable_ops\n",
    "from tensorflow.python.ops import state_ops\n",
    "from tensorflow.python.training import optimizer\n",
    "\n",
    "\n",
    "def generate_batches(data, batch_size=128, shuffle=False):\n",
    "    \"\"\" Generate batches for a data array.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: ndarray\n",
    "        Input data array\n",
    "    batch_size: int\n",
    "        Batch size\n",
    "    shuffle: bool\n",
    "        Flag to shuffle the input data before generating batches if true.\n",
    "    Yields\n",
    "    -------\n",
    "    iterator\n",
    "        The next batch in the sequence of batches of the specified size\n",
    "    \"\"\"\n",
    "    data_size = len(data)\n",
    "    data_idxs = np.array(range(data_size))\n",
    "    if shuffle:\n",
    "        np.random.shuffle(data_idxs)\n",
    "    nb_batches = int(ceil(data_size/batch_size))\n",
    "    for idx in range(nb_batches):\n",
    "        batch_indices = data_idxs[idx * batch_size: min((idx + 1) * batch_size, len(data))]\n",
    "        yield data[batch_indices]\n",
    "\n",
    "\n",
    "def generate_rand_negs(triples, nb_corrs, nb_ents, seed, *args, **kwargs):\n",
    "    \"\"\" Generate random negatives for some positive triples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    triples : tf.Tensor\n",
    "        tensorflow tensor for positive triples with size [?, 3].\n",
    "    nb_corrs : int\n",
    "        Number of corruptions to generate per triple.\n",
    "    nb_ents : int\n",
    "        Total number of entities.\n",
    "    seed : int\n",
    "        Random seed.\n",
    "    Returns\n",
    "    ---------\n",
    "    tf.Tensor\n",
    "        tensorflow tensor for negative triples of size [?, 3].\n",
    "    \"\"\"\n",
    "    nb_corrs /= 2\n",
    "    neg_sub_rel, objs = tf.split(tf.tile(triples, [ceil(nb_corrs), 1]), [2, 1], axis=1)\n",
    "    subs, neg_rel_obj = tf.split(tf.tile(triples, [floor(nb_corrs), 1]), [1, 2], axis=1)\n",
    "\n",
    "    neg_objs = tf.random.uniform(tf.shape(objs), dtype=tf.int32, minval=0, maxval=nb_ents, seed=seed)\n",
    "    neg_subs = tf.random.uniform(tf.shape(subs), dtype=tf.int32, minval=0, maxval=nb_ents, seed=seed)\n",
    "\n",
    "    return tf.concat([tf.concat([neg_subs, neg_rel_obj], axis=1), tf.concat([neg_sub_rel, neg_objs], axis=1)], axis=0)\n",
    "\n",
    "\n",
    "def init_tf_optimiser(optimiser, lr=0.01, *args, **kwargs):\n",
    "    \"\"\" Initialise tensorflow optimiser object\n",
    "    Parameters\n",
    "    ----------\n",
    "    optimiser: str\n",
    "        optimiser name\n",
    "    lr: float\n",
    "        learning rate\n",
    "    args: list\n",
    "        Non-key arguments\n",
    "    kwargs: dict\n",
    "        Key arguments\n",
    "    Returns\n",
    "    -------\n",
    "    Optimizer\n",
    "        tensorflow optimiser\n",
    "    \"\"\"\n",
    "\n",
    "    if optimiser.lower() == \"sgd\":\n",
    "        opt = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "    elif optimiser.lower() == \"adagrad\":\n",
    "        opt = tf.train.AdagradOptimizer(learning_rate=lr)\n",
    "    elif optimiser.lower() == \"adam\":\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    elif optimiser.lower() == \"amsgrad\":\n",
    "        opt = AMSGrad(learning_rate=lr)\n",
    "    elif optimiser.lower() == \"adadelta\":\n",
    "        opt = tf.train.AdadeltaOptimizer(learning_rate=lr)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimiser type (%s).\" % optimiser)\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "class AMSGrad(optimizer.Optimizer):\n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.99, epsilon=1e-8, use_locking=False, name=\"AMSGrad\"):\n",
    "        super(AMSGrad, self).__init__(use_locking, name)\n",
    "        self._lr = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "\n",
    "        self._lr_t = None\n",
    "        self._beta1_t = None\n",
    "        self._beta2_t = None\n",
    "        self._epsilon_t = None\n",
    "\n",
    "        self._beta1_power = None\n",
    "        self._beta2_power = None\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        first_var = min(var_list, key=lambda x: x.name)\n",
    "\n",
    "        create_new = self._beta1_power is None\n",
    "        if not create_new:\n",
    "            create_new = (self._beta1_power.graph is not first_var.graph)\n",
    "\n",
    "        if create_new:\n",
    "            with ops.colocate_with(first_var):\n",
    "                self._beta1_power = tf.Variable(self._beta1, name=\"beta1_power\", trainable=False)\n",
    "                self._beta2_power = tf.Variable(self._beta2, name=\"beta2_power\", trainable=False)\n",
    "        # Create slots for the first and second moments.\n",
    "        for v in var_list :\n",
    "            self._zeros_slot(v, \"m\", self._name)\n",
    "            self._zeros_slot(v, \"v\", self._name)\n",
    "            self._zeros_slot(v, \"vhat\", self._name)\n",
    "\n",
    "    def _prepare(self):\n",
    "        self._lr_t = ops.convert_to_tensor(self._lr)\n",
    "        self._beta1_t = ops.convert_to_tensor(self._beta1)\n",
    "        self._beta2_t = ops.convert_to_tensor(self._beta2)\n",
    "        self._epsilon_t = ops.convert_to_tensor(self._epsilon)\n",
    "\n",
    "    def _apply_dense(self, grad, var):\n",
    "        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "        v_sqrt = math_ops.sqrt(vhat_t)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var):\n",
    "        var = var.handle\n",
    "        beta1_power = math_ops.cast(self._beta1_power, grad.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._beta2_power, grad.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, grad.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, grad.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, grad.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, grad.dtype.base_dtype)\n",
    "\n",
    "        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\").handle\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, beta1_t * m + m_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\").handle\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, beta2_t * v + v_scaled_g_values, use_locking=self._use_locking)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\").handle\n",
    "        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "        v_sqrt = math_ops.sqrt(vhat_t)\n",
    "\n",
    "        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n",
    "        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n",
    "        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n",
    "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
    "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
    "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
    "        epsilon_t = math_ops.cast(self._epsilon_t, var.dtype.base_dtype)\n",
    "\n",
    "        lr = (lr_t * math_ops.sqrt(1 - beta2_power) / (1 - beta1_power))\n",
    "\n",
    "        # m_t = beta1 * m + (1 - beta1) * g_t\n",
    "        m = self.get_slot(var, \"m\")\n",
    "        m_scaled_g_values = grad * (1 - beta1_t)\n",
    "        m_t = state_ops.assign(m, m * beta1_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([m_t]):\n",
    "            m_t = scatter_add(m, indices, m_scaled_g_values)\n",
    "\n",
    "        # v_t = beta2 * v + (1 - beta2) * (g_t * g_t)\n",
    "        v = self.get_slot(var, \"v\")\n",
    "        v_scaled_g_values = (grad * grad) * (1 - beta2_t)\n",
    "        v_t = state_ops.assign(v, v * beta2_t, use_locking=self._use_locking)\n",
    "        with ops.control_dependencies([v_t]):\n",
    "            v_t = scatter_add(v, indices, v_scaled_g_values)\n",
    "\n",
    "        # amsgrad\n",
    "        vhat = self.get_slot(var, \"vhat\")\n",
    "        vhat_t = state_ops.assign(vhat, math_ops.maximum(v_t, vhat))\n",
    "        v_sqrt = math_ops.sqrt(vhat_t)\n",
    "        var_update = state_ops.assign_sub(var, lr * m_t / (v_sqrt + epsilon_t), use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*[var_update, m_t, v_t, vhat_t])\n",
    "\n",
    "    def _apply_sparse(self, grad, var):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad.values, var, grad.indices,\n",
    "            lambda x, i, v: state_ops.scatter_add(  # pylint: disable=g-long-lambda\n",
    "                x, i, v, use_locking=self._use_locking))\n",
    "\n",
    "    def _resource_scatter_add(self, x, i, v):\n",
    "        with ops.control_dependencies(\n",
    "                [resource_variable_ops.resource_scatter_add(x.handle, i, v)]):\n",
    "            return x.value()\n",
    "\n",
    "    def _resource_apply_sparse(self, grad, var, indices):\n",
    "        return self._apply_sparse_shared(\n",
    "            grad, var, indices, self._resource_scatter_add)\n",
    "\n",
    "    def _finish(self, update_ops, name_scope):\n",
    "        # Update the power accumulators.\n",
    "        with ops.control_dependencies(update_ops):\n",
    "            with ops.colocate_with(self._beta1_power):\n",
    "                update_beta1 = self._beta1_power.assign(\n",
    "                    self._beta1_power * self._beta1_t,\n",
    "                    use_locking=self._use_locking)\n",
    "                update_beta2 = self._beta2_power.assign(\n",
    "                    self._beta2_power * self._beta2_t,\n",
    "                    use_locking=self._use_locking)\n",
    "        return control_flow_ops.group(*update_ops + [update_beta1, update_beta2],\n",
    "                                      name=name_scope)\n",
    "\n",
    "\n",
    "def log_model_params(model_obj):\n",
    "    \"\"\" Log model parameters\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_obj: KnowledgeGraphEmbeddingModel\n",
    "        KGE model object\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    model_attrs = model_obj.__dict__\n",
    "    for attr in sorted(list(model_attrs.keys())):\n",
    "        if type(model_attrs[attr]) == str or type(model_attrs[attr]) == int or type(model_attrs[attr]) == float or \\\n",
    "                type(model_attrs[attr]) == bool:\n",
    "            attr_val = model_attrs[attr]\n",
    "            model_obj.log.debug(\"[Parameter] %-20s: %s\" % (attr, attr_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
