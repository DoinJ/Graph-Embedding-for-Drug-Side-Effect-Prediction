{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86902b06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from abc import ABCMeta, abstractmethod\n",
    "import logging\n",
    "from sys import stdout\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import Ipynb_importer\n",
    "from constraints import *\n",
    "from losses import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e671755",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KnowledgeGraphEmbeddingModel(BaseEstimator, RegressorMixin, metaclass=ABCMeta):\n",
    "    \"\"\"An abstract class for the model.\n",
    "    Attributes\n",
    "    ----------\n",
    "    em_size: int, optional\n",
    "            embedding vector size\n",
    "    batch_size: int\n",
    "        batch size\n",
    "    nb_epochs: int\n",
    "        number of epoch i.e training iterations\n",
    "    initialiser: str\n",
    "        initialiser name e.g. xavier_uniform or he_normal\n",
    "    nb_negs: int\n",
    "        number of negative instance per each positive training instance\n",
    "    optimiser: str\n",
    "        optimiser name\n",
    "    lr: float\n",
    "        optimiser learning rate\n",
    "    loss: str\n",
    "        loss type e.g. pt_logistic or pt_se.\n",
    "    nb_ents: int\n",
    "        total number of knowledge graph entities\n",
    "    nb_rels: int\n",
    "        total number of knowldege graph relations\n",
    "    reg_wt: float\n",
    "        regularisation parameter weight\n",
    "    seed: int\n",
    "        random seed\n",
    "    verbose: int\n",
    "        verbosity level. options are {0, 1, 2}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, em_size=100, batch_size=128, nb_epochs=100, initialiser=\"xavier_uniform\", nb_negs=2,\n",
    "                 optimiser=\"amsgrad\", lr=0.01, loss=\"pt_logistic\", nb_ents=-1, nb_rels=-1, reg_wt=0.01,\n",
    "                 predict_batch_size=40000, seed=1234, verbose=1, log_interval=5):\n",
    "        \"\"\" Initialize the instances.\n",
    "        Parameters\n",
    "        ----------\n",
    "        em_size: int\n",
    "            embedding vector size\n",
    "        batch_size: int\n",
    "            batch size\n",
    "        nb_epochs: int\n",
    "            number of training iterations\n",
    "        initialiser: str\n",
    "            initialiser name e.g. xavier_uniform or he_normal\n",
    "        nb_negs: int\n",
    "            number of negative instance per each positive training instance\n",
    "        optimiser: str\n",
    "            optimiser name\n",
    "        lr: float\n",
    "            optimiser learning rate\n",
    "        loss: str\n",
    "            loss type e.g. pt_logistic or pt_se.\n",
    "        nb_ents: int\n",
    "            total number of knowledge graph entities\n",
    "        nb_rels: int\n",
    "            total number of knowledge graph relations\n",
    "        reg_wt: float\n",
    "            regularisation parameter weight\n",
    "        predict_batch_size : int\n",
    "            batch size in prediction mode\n",
    "        seed: int\n",
    "            random seed\n",
    "        verbose: int\n",
    "            verbosity level. options are {0, 1, 2}\n",
    "        log_interval: int\n",
    "            the number of epochs to wait until reporting the next training loss. (loss logging frequency)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.em_size = em_size\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.initialiser = initialiser\n",
    "        self.nb_negs = nb_negs\n",
    "        self.optimiser = optimiser\n",
    "        self.lr = lr\n",
    "        self.loss = loss\n",
    "        self.nb_ents = nb_ents\n",
    "        self.nb_rels = nb_rels\n",
    "        self.reg_wt = reg_wt\n",
    "        self.predict_batch_size = predict_batch_size\n",
    "        self.log_interval = log_interval\n",
    "\n",
    "        # init tf related vars\n",
    "        self._embeddings = dict()\n",
    "        self._tf_vars = dict()\n",
    "        self._predict_pipeline_on = False\n",
    "\n",
    "        # logging - initialise multiple verbosity logger\n",
    "        self.log = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # random states - initialise numpy and tensorflow random seeds\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        tf.random.set_seed(seed)\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\" This is called before pickling. \"\"\"\n",
    "        _blacklist = ['tf_vars_', 'trainable_vars_', '_tf_session', '_tf_session_config', '_logger', '_random_state']\n",
    "        accepted_types = {int, float, str, bool}\n",
    "        state = {k: v for k, v in self.__dict__.items() if type(k) in accepted_types}\n",
    "        # print('GET STATE: %s' % state)\n",
    "        return state\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        \"\"\" This is called while unpickling.\n",
    "        \"\"\"\n",
    "        # print('SET STATE: %s' % state)\n",
    "        self.__dict__.update(state)\n",
    "        if 'verbose' in state.keys():\n",
    "            self.init_logging(state['verbose'])\n",
    "        if 'seed' in state.keys():\n",
    "            np.random.seed(self.seed)\n",
    "            tf.random.set_seed(self.seed)\n",
    "            self.rs = np.random.RandomState(seed=self.seed)\n",
    "        # init some variables for tensorflow\n",
    "        self._init_tf_session()\n",
    "\n",
    "    def init_logging(self, verbose=0):\n",
    "        \"\"\" Initialise class logger with specified verbosity.\n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose : int\n",
    "            verbosity level.\n",
    "        Note\n",
    "        ----------\n",
    "        Based on an answer on Stackoverflow question:\n",
    "        https://stackoverflow.com/questions/11927278/how-to-configure-logging-in-python/11927374\n",
    "        \"\"\"\n",
    "        self.log = logging.getLogger(self.__class__.__name__)\n",
    "        if verbose == 0:\n",
    "            self.log.setLevel(logging.WARN)\n",
    "        elif verbose == 1:\n",
    "            self.log.setLevel(logging.INFO)\n",
    "        elif verbose == 2:\n",
    "            self.log.setLevel(logging.DEBUG)\n",
    "        else:\n",
    "            self.log.critical('Unknown verbosity level (%d). Options are [0, 1, 2]' % verbose)\n",
    "\n",
    "        # important for parallel processing\n",
    "        self.log.propagate = False\n",
    "\n",
    "        if not self.log.handlers:\n",
    "            console_handler = logging.StreamHandler(stdout)\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            console_handler.setFormatter(formatter)\n",
    "            console_handler.setLevel(self.log.level)\n",
    "            self.log.addHandler(console_handler)\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        \"\"\" Initialise model _embeddings for both entities and relations\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Variable\n",
    "            _embeddings of knowledge graph entities\n",
    "        tf.Variable\n",
    "            _embeddings of knowledge graph relations\n",
    "        \"\"\"\n",
    "        # get initialiser variable and initialise tensorflow variables for each component _embeddings\n",
    "        var_init = get_initializer(self.initialiser, self.seed)\n",
    "        em_ents = tf.compat.v1.get_variable(\"em_ents\", shape=[self.nb_ents+1, self.em_size], initializer=var_init)\n",
    "        em_rels = tf.compat.v1.get_variable(\"em_rels\", shape=[self.nb_rels+1, self.em_size], initializer=var_init)\n",
    "\n",
    "        # add component embedding to the embedding vars dictionary\n",
    "        self._embeddings[\"ents\"] = em_ents\n",
    "        self._embeddings[\"rels\"] = em_rels\n",
    "\n",
    "        return em_ents, em_rels\n",
    "\n",
    "    def lookup_triples_embeddings(self, triples):\n",
    "        \"\"\" Lookup triple _embeddings\n",
    "        Parameters\n",
    "        ----------\n",
    "        triples: tf.tensor\n",
    "            tensorflow tensor of size [?, 3]\n",
    "        Returns\n",
    "        -------\n",
    "        tf.tensor\n",
    "            Embeddings of the subject entities\n",
    "        tf.tensor\n",
    "            Embeddings of the relations\n",
    "        tf.tensor\n",
    "            Embeddings of the object entities\n",
    "        \"\"\"\n",
    "        subs_em = tf.nn.embedding_lookup(self._embeddings[\"ents\"], triples[:, 0])\n",
    "        rels_em = tf.nn.embedding_lookup(self._embeddings[\"rels\"], triples[:, 1])\n",
    "        objs_em = tf.nn.embedding_lookup(self._embeddings[\"ents\"], triples[:, 2])\n",
    "\n",
    "        return subs_em, rels_em, objs_em\n",
    "\n",
    "    def generate_negatives(self, triples, *args, **kwargs):\n",
    "        \"\"\" Generate negative triple corruptions.\n",
    "        Parameters\n",
    "        ----------\n",
    "        triples: tf.tensor\n",
    "            (N, 3) tensorflow tensor of original true triples.\n",
    "        args : list\n",
    "            Non-key arguments\n",
    "        kwargs : dict\n",
    "            Key arguments\n",
    "        Returns\n",
    "        -------\n",
    "        tf.tensor\n",
    "            (N, 3) tensorflow tensor with negative triples.\n",
    "        \"\"\"\n",
    "        return generate_rand_negs(triples, self.nb_negs, self.nb_ents, self.seed, *args, **kwargs)\n",
    "\n",
    "    def embedding_normalisation(self, *args, **kwargs):\n",
    "        \"\"\" Execute post optimisation embedding normalisation.\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : list\n",
    "            Non-key arguments.\n",
    "        kwargs : dict\n",
    "            Key arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list of embedding normalisation operations.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @abstractmethod\n",
    "    def score_triples(self, sub_em, rel_em, obj_em):\n",
    "        \"\"\" Compute scores of triples using the _embeddings of their components\n",
    "        Parameters\n",
    "        ----------\n",
    "        sub_em: tf.tensor\n",
    "            Embeddings of the subject entities\n",
    "        rel_em: tf.tensor\n",
    "            Embeddings of the relations\n",
    "        obj_em: tf.tensor\n",
    "            Embeddings of the object entities\n",
    "        Returns\n",
    "        -------\n",
    "        tf.tensor\n",
    "            Scores of the original triples of the given components _embeddings\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Not implemented model dependant function\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_loss(self, scores, *args, **kwargs):\n",
    "        \"\"\" Model dependant loss function.\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores: tf.tensor\n",
    "            (N,) tensorflow tensor with all batch triples scores.\n",
    "        args : list\n",
    "            Non-key arguments.\n",
    "        kwargs : dict\n",
    "            Key arguments.\n",
    "        Returns\n",
    "        -------\n",
    "        tf.float32\n",
    "            Model loss.\n",
    "        \"\"\"\n",
    "        # set pairwise hinge loss to be the default loss\n",
    "        return compute_kge_loss(scores, loss_type=\"pr_hinge\", *args, **kwargs)\n",
    "\n",
    "    def embedding_regularisation(self, sub_em, rel_em, obj_em, *args, **kwargs):\n",
    "        \"\"\" Compute embedding regularisation term\n",
    "        Parameters\n",
    "        ----------\n",
    "        sub_em: tf.tensor\n",
    "            Embeddings of the subject entities\n",
    "        rel_em: tf.tensor\n",
    "            Embeddings of the relations\n",
    "        obj_em: tf.tensor\n",
    "            Embeddings of the object entities\n",
    "        args: list\n",
    "            Non-key arguments\n",
    "        kwargs: dict\n",
    "            Key arguments\n",
    "        Returns\n",
    "        -------\n",
    "        tf.float32\n",
    "            regularisation value\n",
    "        \"\"\"\n",
    "        # set the regularisation function to zero by default. (disabled)\n",
    "        reg_val = 0\n",
    "        return reg_val * self.reg_wt\n",
    "\n",
    "    def fit(self, X, y=None, *args, **kwargs):\n",
    "        \"\"\" Train the model on the given input triples\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray\n",
    "            input triples array of size [?, 3]\n",
    "        y: ndarray\n",
    "            input triplets labels.\n",
    "        args: list\n",
    "            unnamed arguments\n",
    "        kwargs: dict\n",
    "            named arguments\n",
    "        \"\"\"\n",
    "\n",
    "        # initialise the model's logger\n",
    "        self.init_logging(self.verbose)\n",
    "\n",
    "        self._predict_pipeline_on = False\n",
    "        self.log.debug(\"Logging model parameters ...\")\n",
    "        # log model parameters in debug mode\n",
    "        log_model_params(self)\n",
    "        self.log.debug(\"Model training started ...\")\n",
    "        # compute the number of entities and relations if not given\n",
    "        train_size = len(X)\n",
    "        if self.nb_ents is None or self.nb_ents <= 0:\n",
    "            entities_vocab = np.unique(np.concatenate([X[:, 0], X[:, 2]], axis=0))\n",
    "            self.nb_ents = int(max(entities_vocab) + 1)\n",
    "\n",
    "        if self.nb_rels is None or self.nb_rels <= 0:\n",
    "            relations_vocab = np.unique(X[:, 1])\n",
    "            self.nb_rels = int(max(relations_vocab) + 1)\n",
    "\n",
    "        self.log.debug(\"Training model [ %d #Instances - %d #Entities - %d #Relations ]\"\n",
    "                       % (train_size, self.nb_ents, self.nb_rels))\n",
    "\n",
    "        # ================================================================================================\n",
    "        # tensorflow graph for the embedding model\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "        # initialise model _embeddings\n",
    "        self.init_embeddings()\n",
    "\n",
    "        # define input placeholder\n",
    "        self._tf_vars[\"xin_pos\"] = xin_pos = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
    "\n",
    "        # generate negative corruption from the input triples\n",
    "        xin_neg = self.generate_negatives(xin_pos)\n",
    "\n",
    "        # tile positive triples and join them with negatives to make the batch triples classes balanced and divisible\n",
    "        # from the middle into positive and negative classes\n",
    "        xin_all = tf.concat([tf.tile(xin_pos, [self.nb_negs, 1]), xin_neg], axis=0)\n",
    "\n",
    "        # lookup embedding of the triples components\n",
    "        em_subs, em_rels, em_objs = self.lookup_triples_embeddings(xin_all)\n",
    "\n",
    "        # compute triples' scores\n",
    "        self._tf_vars[\"scores\"] = scores = self.score_triples(em_subs, em_rels, em_objs)\n",
    "\n",
    "        # compute regularisation for components _embeddings\n",
    "        reg_term = self.embedding_regularisation(em_subs, em_rels, em_objs)\n",
    "\n",
    "        # compute model loss and objective cost\n",
    "        self._tf_vars[\"loss\"] = model_loss = self.compute_loss(scores)\n",
    "        model_train_error = model_loss + reg_term\n",
    "\n",
    "        # initialise optimiser and minimise training error\n",
    "        optimiser = init_tf_optimiser(self.optimiser, self.lr)\n",
    "        optimisation = optimiser.minimize(model_train_error)\n",
    "\n",
    "        # execute embedding normalisation procedure\n",
    "        exec_norm = self.embedding_normalisation()\n",
    "        # ================================================================================================\n",
    "\n",
    "        self.log.debug(\"Initialising tensorflow session\")\n",
    "        session = self._init_tf_session()\n",
    "        self.log.debug(\"Executing tensorflow global variable initialiser\")\n",
    "        session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        tr_loss_list = []\n",
    "        tr_speed_list = []\n",
    "        for epoch in range(self.nb_epochs):\n",
    "            train_batches = generate_batches(X, batch_size=self.batch_size, shuffle=True)\n",
    "            epoch_loss_list = []\n",
    "            epoch_tr_start_time = time()\n",
    "            for batch_idx, batch_data in enumerate(train_batches):\n",
    "                arg_dict = {xin_pos: batch_data}\n",
    "                batch_tr_loss, _ = session.run([model_loss, optimisation], feed_dict=arg_dict)\n",
    "                session.run(exec_norm)\n",
    "                epoch_loss_list.append(batch_tr_loss)\n",
    "                tr_loss_list.append(batch_tr_loss)\n",
    "            epoch_tr_end_time = time()\n",
    "            epoch_tr_time = train_size / (epoch_tr_end_time - epoch_tr_start_time)\n",
    "            epoch_tr_time /= 1000.0\n",
    "            tr_speed_list.append(epoch_tr_time)\n",
    "            epoch_loss_avg = np.mean(epoch_loss_list)\n",
    "            if epoch == 0 or (epoch+1) % self.log_interval == 0:\n",
    "                self.log.debug(\"[Training] Epoch # %-4d - Speed: %1.3f (k. record/sec) - Loss: %-4.4f \"\n",
    "                               \"- Avg(Loss): %-4.4f - Std(Loss): %-4.4f\" %\n",
    "                               (epoch+1, epoch_tr_time, epoch_loss_avg, np.mean(tr_loss_list), np.std(tr_loss_list)))\n",
    "        self.log.debug(\"[Reporting] Finished (%d Epochs) - Avg(Speed): %1.3f (k. record/sec) \"\n",
    "                       \"- Avg(Loss): %-4.4f - Std(Loss): %-4.4f\" %\n",
    "                       (self.nb_epochs, np.mean(tr_speed_list), np.mean(tr_loss_list), np.std(tr_loss_list)))\n",
    "\n",
    "    def _init_tf_session(self):\n",
    "        tf_session_config = tf.compat.v1.ConfigProto(log_device_placement=False, allow_soft_placement=True)\n",
    "        tf_session_config.gpu_options.allow_growth = True\n",
    "        self._tf_vars[\"session\"] = session = tf.compat.v1.Session(config=tf_session_config)\n",
    "        return session\n",
    "\n",
    "    def _init_prediction_flow(self):\n",
    "        \"\"\" Initialise the tensorflow graph for predicting new triples\n",
    "        Returns\n",
    "        ----------\n",
    "        tf.Placeholder\n",
    "            the input triplets placeholder\n",
    "        tf.Tensor\n",
    "            the scores tensorflow tensor\n",
    "        \"\"\"\n",
    "        if not (\"session\" in self._tf_vars and type(self._tf_vars[\"session\"]) == tf.compat.v1.Session):\n",
    "            self._init_tf_session()\n",
    "        self._tf_vars[\"xin_predict\"] = xin_predict = tf.compat.v1.placeholder(dtype=tf.int32, shape=[None, 3])\n",
    "        # lookup embedding of the triples components\n",
    "        em_subs, em_rels, em_objs = self.lookup_triples_embeddings(xin_predict)\n",
    "        # compute triples' scores\n",
    "        self._tf_vars[\"scores_predict\"] = scores = self.score_triples(em_subs, em_rels, em_objs)\n",
    "        self._predict_pipeline_on = True\n",
    "        return xin_predict, scores\n",
    "\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        \"\"\" Predict scores of a set of knowledge triples.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: ndarray\n",
    "            input triples array of size [?, 3]\n",
    "        args: list\n",
    "            unnamed arguments\n",
    "        kwargs: dict\n",
    "            named arguments\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            outcome scores of input triples\n",
    "        \"\"\"\n",
    "\n",
    "        # initialise the prediction flow if not initialised\n",
    "        if not self._predict_pipeline_on:\n",
    "            self._init_prediction_flow()\n",
    "        xin, scores_tf, session = self._tf_vars[\"xin_predict\"], self._tf_vars[\"scores_predict\"], self._tf_vars[\"session\"]\n",
    "\n",
    "        predict_batches = generate_batches(X, batch_size=self.predict_batch_size, shuffle=False)\n",
    "        output_scores = []\n",
    "        for batch_data in predict_batches:\n",
    "            batch_scores = session.run(scores_tf, feed_dict={xin: batch_data})\n",
    "            output_scores.extend(batch_scores.tolist())\n",
    "        return np.array(output_scores)\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        \"\"\" Get model learnt embeddings\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            dictionary of embeddings\n",
    "        \"\"\"\n",
    "        em_dict = dict()\n",
    "        for k, v in self._embeddings.items():\n",
    "            em_dict[k] = self._tf_vars[\"session\"].run(v)\n",
    "        return em_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
