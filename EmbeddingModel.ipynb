{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Ipynb_importer\n",
    "from losses import *\n",
    "from constraints import get_initializer\n",
    "from base_model import KnowledgeGraphEmbeddingModel\n",
    "\n",
    "       \n",
    "        \n",
    "class Cnmsb(KnowledgeGraphEmbeddingModel):\n",
    "    \"\"\"\n",
    "    The embedding model (Cnmsb)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, em_size=100, batch_size=128, nb_epochs=100, initialiser=\"xavier_uniform\", nb_negs=2,\n",
    "                 optimiser=\"amsgrad\", loss=\"default\", lr=0.01, nb_ents=0, nb_rels=0, reg_wt=0.01, seed=1234, verbose=1,\n",
    "                 log_interval=5):\n",
    "        \"\"\" Initialise new instance of the ComplEx model\n",
    "        Parameters\n",
    "        ----------\n",
    "        em_size: int\n",
    "            embedding vector size\n",
    "        batch_size: int\n",
    "            batch size\n",
    "        nb_epochs: int\n",
    "            number of epoch i.e training iterations\n",
    "        initialiser: str\n",
    "            initialiser name e.g. xavier_uniform or he_normal\n",
    "        nb_negs: int\n",
    "            number of negative instance per each positive training instance\n",
    "        optimiser: str\n",
    "            optimiser name\n",
    "        lr: float\n",
    "            optimiser learning rate\n",
    "        nb_ents: int\n",
    "            total number of knowledge graph entities\n",
    "        nb_rels: int\n",
    "            total number of knowledge graph relations\n",
    "        reg_wt: float\n",
    "            regularisation parameter weight\n",
    "        seed: int\n",
    "            random seed\n",
    "        verbose: int\n",
    "            verbosity level. options are {0, 1, 2}\n",
    "        log_interval: int\n",
    "            the number of epochs to wait until reporting the next training loss. (loss logging frequency)\n",
    "        \"\"\"\n",
    "        super().__init__(em_size=em_size, batch_size=batch_size, nb_epochs=nb_epochs, initialiser=initialiser,\n",
    "                         nb_negs=nb_negs, optimiser=optimiser, lr=lr, loss=loss, nb_ents=nb_ents, nb_rels=nb_rels,\n",
    "                         reg_wt=reg_wt, seed=seed, verbose=verbose, log_interval=log_interval)\n",
    "\n",
    "    def init_embeddings(self):\n",
    "        \"\"\" Initialise the embeddings for both entities and relations.\n",
    "        Returns\n",
    "        -------\n",
    "        tf.Variable\n",
    "            _embeddings of knowledge graph entities\n",
    "        tf.Variable\n",
    "            _embeddings of knowledge graph relations\n",
    "        \"\"\"\n",
    "        # get initialiser variable and initialise tensorflow variables for each component _embeddings\n",
    "        var_init = get_initializer(self.initialiser, self.seed)\n",
    "        em_ents = tf.compat.v1.get_variable(\"em_ents\", shape=[self.nb_ents+1, self.em_size*2], initializer=var_init)\n",
    "        em_rels = tf.compat.v1.get_variable(\"em_rels\", shape=[self.nb_rels+1, self.em_size*2], initializer=var_init)\n",
    "\n",
    "        # add component embedding to the embedding vars dictionary\n",
    "        self._embeddings[\"ents\"] = em_ents\n",
    "        self._embeddings[\"rels\"] = em_rels\n",
    "\n",
    "        return em_ents, em_rels\n",
    "\n",
    "    def score_triples(self, sub_em, rel_em, obj_em):\n",
    "        \"\"\" Compute scores for a set of triples given their component _embeddings\n",
    "        Parameters\n",
    "        ----------\n",
    "        sub_em: tf.tensor\n",
    "            Embeddings of the subject entities\n",
    "        rel_em: tf.tensor\n",
    "            Embeddings of the relations\n",
    "        obj_em: tf.tensor\n",
    "            Embeddings of the object entities\n",
    "        Returns\n",
    "        -------\n",
    "        tf.tensor\n",
    "            model scores for the original triples of the given _embeddings\n",
    "        \"\"\"\n",
    "        em_interactions = sub_em * rel_em * obj_em\n",
    "        scores = tf.reduce_sum(em_interactions, axis=1)\n",
    "        return scores\n",
    "\n",
    "    \n",
    "    def compute_loss(self, scores, *args, **kwargs):\n",
    "        \"\"\" Compute training loss using the loss function\n",
    "        Parameters\n",
    "        ----------\n",
    "        scores: tf.Tenor\n",
    "            scores tensor\n",
    "        args: list\n",
    "            Non-Key arguments\n",
    "        kwargs: dict\n",
    "            Key arguments\n",
    "        Returns\n",
    "        -------\n",
    "        tf.float32\n",
    "            model loss value\n",
    "        \"\"\"\n",
    "        if self.loss == \"default\":\n",
    "            pos_scores, neg_scores = tf.split(scores, num_or_size_splits=2)\n",
    "            targets = tf.concat((tf.ones(tf.shape(pos_scores)), -1 * tf.ones(tf.shape(neg_scores))), axis=0)\n",
    "            return pointwise_logistic_loss(scores, targets=targets, reduction_type=\"avg\")\n",
    "        else:\n",
    "            return compute_kge_loss(scores, self.loss, reduction_type=\"avg\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
